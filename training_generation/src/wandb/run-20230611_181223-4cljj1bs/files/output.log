/scratch/venia/python/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|                                                                                                                                                                                   | 0/110 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/scratch/venia/python/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

 10%|█████████████████                                                                                                                                                         | 11/110 [00:11<00:38,  2.60it/s]
{'loss': 0.686, 'learning_rate': 1.8e-05, 'epoch': 1.0}

 10%|█████████████████                                                                                                                                                         | 11/110 [00:11<00:38,  2.60it/s]Error executing job with overrides: []
Traceback (most recent call last):
  File "/scratch/venia/socialgpt/venia_worker_vs_gpt/src/finetune_model.py", line 142, in main
    model.train()
  File "/scratch/venia/socialgpt/venia_worker_vs_gpt/src/finetuning/trainers.py", line 181, in train
    trainer.train()
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/transformers/trainer.py", line 2021, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/transformers/trainer.py", line 2291, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/transformers/trainer.py", line 2394, in _save_checkpoint
    metric_value = metrics[metric_to_check]
KeyError: 'eval_loss'
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.