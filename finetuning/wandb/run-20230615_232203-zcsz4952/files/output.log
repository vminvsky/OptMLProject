
Epoch: 0, Loss:  0.666945219039917
Epoch: 0, Loss:  0.6934817433357239
Epoch: 0, Loss:  0.6675056219100952
Epoch: 0, Loss:  0.6967616081237793
Epoch: 0, Loss:  0.6935296058654785
Epoch: 0, Loss:  0.6832020282745361
Epoch: 0, Loss:  0.6544468998908997
Epoch: 0, Loss:  0.6683796644210815
Epoch: 0, Loss:  0.6831270456314087
Epoch: 0, Loss:  0.671012818813324
Epoch: 0, Loss:  0.6714534163475037
Epoch: 0, Loss:  0.6764678955078125
Epoch: 0, Loss:  0.6809263825416565
Epoch: 0, Loss:  0.7033537030220032
Epoch: 0, Loss:  0.6814244389533997
Epoch: 0, Loss:  0.6904959678649902
Epoch: 0, Loss:  0.6862612962722778
Epoch: 0, Loss:  0.6733524799346924
Epoch: 0, Loss:  0.6814805269241333
Epoch: 0, Loss:  0.6938828229904175
Epoch: 0, Loss:  0.6764461994171143
Epoch: 0, Loss:  0.6580524444580078
Epoch: 0, Loss:  0.6736758351325989
Epoch: 0, Loss:  0.6882179975509644
Epoch: 0, Loss:  0.7111343741416931
Epoch: 0, Loss:  0.6692954897880554
Epoch: 0, Loss:  0.6699763536453247
Epoch: 0, Loss:  0.6800302863121033
Epoch: 0, Loss:  0.6667683124542236
Epoch: 0, Loss:  0.7043461203575134
Epoch: 0, Loss:  0.6702573299407959
Epoch: 0, Loss:  0.6994380950927734
Epoch: 0, Loss:  0.665608286857605
Epoch: 0, Loss:  0.6811512112617493
Epoch: 0, Loss:  0.6795129776000977
Epoch: 0, Loss:  0.686856210231781
Epoch: 0, Loss:  0.7062806487083435
Epoch: 0, Loss:  0.6678987741470337
Epoch: 0, Loss:  0.6947870254516602
Epoch: 0, Loss:  0.689116358757019
Epoch: 0, Loss:  0.684751570224762
Epoch: 0, Loss:  0.6710129976272583
Epoch: 0, Loss:  0.6731512546539307
Epoch: 0, Loss:  0.6646717190742493
Epoch: 0, Loss:  0.6863179802894592
Epoch: 0, Loss:  0.6655195951461792
Epoch: 0, Loss:  0.6848888397216797
Epoch: 0, Loss:  0.6844526529312134
Epoch: 0, Loss:  0.6944330930709839
Epoch: 0, Loss:  0.6706175804138184
Epoch: 0, Loss:  0.6646196842193604
Epoch: 0, Loss:  0.6807359457015991
Traceback (most recent call last):
  File "/scratch/venia/OptMLProject/finetuning/finetuning.py", line 252, in <module>
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/scratch/venia/OptMLProject/finetuning/finetuning.py", line 215, in main
    loss.backward()
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/scratch/venia/OptMLProject/finetuning/optimizers/sam.py", line 46, in step
    closure()
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/scratch/venia/OptMLProject/finetuning/finetuning.py", line 209, in closure
    return loss
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt