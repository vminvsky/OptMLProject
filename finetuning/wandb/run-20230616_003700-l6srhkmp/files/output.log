Epoch: 0, Loss:  0.666945219039917
Epoch: 0, Loss:  0.6934492588043213
Epoch: 0, Loss:  0.6658369302749634
Epoch: 0, Loss:  0.6901399493217468
Epoch: 0, Loss:  0.6849071979522705
Epoch: 0, Loss:  0.6621410250663757
Epoch: 0, Loss:  0.6917053461074829
Epoch: 0, Loss:  0.6720454692840576
Epoch: 0, Loss:  0.6699934601783752
Epoch: 0, Loss:  0.6730090379714966
Epoch: 0, Loss:  0.6508693695068359
Epoch: 0, Loss:  0.6735838651657104
Epoch: 0, Loss:  0.6987961530685425
Epoch: 0, Loss:  0.7134073376655579
Epoch: 0, Loss:  0.6813739538192749
Epoch: 0, Loss:  0.6825122833251953
Epoch: 0, Loss:  0.6974626779556274
Epoch: 0, Loss:  0.6758539080619812
Epoch: 0, Loss:  0.6929948329925537
Epoch: 0, Loss:  0.6988720893859863
Epoch: 0, Loss:  0.6730765104293823
Epoch: 0, Loss:  0.659939169883728
Epoch: 0, Loss:  0.6667413711547852
Epoch: 0, Loss:  0.6863275170326233
Epoch: 0, Loss:  0.6818000078201294
Epoch: 0, Loss:  0.6663012504577637
Epoch: 0, Loss:  0.664617121219635
Epoch: 0, Loss:  0.6951972842216492
Epoch: 0, Loss:  0.6748154759407043
Epoch: 0, Loss:  0.6958388090133667
Epoch: 0, Loss:  0.6720153093338013
Epoch: 0, Loss:  0.6938353776931763
Epoch: 0, Loss:  0.6766637563705444
Epoch: 0, Loss:  0.6673952341079712
Epoch: 0, Loss:  0.6953018307685852
Epoch: 0, Loss:  0.6538474559783936
Epoch: 0, Loss:  0.6873561143875122
Epoch: 0, Loss:  0.6847642660140991
Epoch: 0, Loss:  0.6648386716842651
Epoch: 0, Loss:  0.6896520853042603
Epoch: 0, Loss:  0.6918531060218811
Epoch: 0, Loss:  0.6752369403839111
Epoch: 0, Loss:  0.6659180521965027
Epoch: 0, Loss:  0.6837724447250366
Epoch: 0, Loss:  0.7024000287055969
Epoch: 0, Loss:  0.6678102016448975
Epoch: 0, Loss:  0.6906436681747437
Epoch: 0, Loss:  0.6680375933647156
Epoch: 0, Loss:  0.6884683966636658
Epoch: 0, Loss:  0.6868070960044861
Epoch: 0, Loss:  0.6862089037895203
Epoch: 0, Loss:  0.6906004548072815
Epoch: 0, Loss:  0.6881831288337708
Epoch: 0, Loss:  0.6681123971939087
Epoch: 0, Loss:  0.6925126314163208
Epoch: 0, Loss:  0.6634606719017029
Epoch: 0, Loss:  0.6725721955299377
Epoch: 0, Loss:  0.6713539958000183
Epoch: 0, Loss:  0.6687414646148682
Epoch: 0, Loss:  0.6978933811187744
Epoch: 0, Loss:  0.7032657861709595
Epoch: 0, Loss:  0.6906534433364868
Epoch: 0, Loss:  0.7034673690795898
Epoch: 0, Loss:  0.6527718305587769
Epoch: 0, Loss:  0.6738024950027466
Epoch: 0, Loss:  0.6802318096160889
Epoch: 0, Loss:  0.6846749186515808
Epoch: 0, Loss:  0.6867392063140869
Epoch: 0, Loss:  0.7019078731536865
Epoch: 0, Loss:  0.6839496493339539
Epoch: 0, Loss:  0.6499805450439453
Epoch: 0, Loss:  0.708711564540863
Epoch: 0, Loss:  0.6878842711448669
Epoch: 0, Loss:  0.6642864942550659
Epoch: 0, Loss:  0.6702374815940857
Epoch: 0, Loss:  0.6897777318954468
Epoch: 0, Loss:  0.6541900634765625
Epoch: 0, Loss:  0.680681049823761
Epoch: 0, Loss:  0.6805559396743774
Epoch: 0, Loss:  0.6765460968017578
Epoch: 0, Loss:  0.6821116209030151
Epoch: 0, Loss:  0.6832018494606018
Epoch: 0, Loss:  0.7086845636367798
Epoch: 0, Loss:  0.6752623319625854
Epoch: 0, Loss:  0.6672985553741455
Epoch: 0, Loss:  0.6926620006561279
Epoch: 0, Loss:  0.670518159866333
Epoch: 0, Loss:  0.691070556640625
Epoch: 0, Loss:  0.6850696802139282
Epoch: 0, Loss:  0.6617152690887451
Epoch: 0, Loss:  0.6736961603164673
Epoch: 0, Loss:  0.6798388957977295
Epoch: 0, Loss:  0.7050824761390686
Epoch: 0, Loss:  0.7051420211791992
Epoch: 0, Loss:  0.6754962205886841
Epoch: 0, Loss:  0.69907546043396
Traceback (most recent call last):
  File "/scratch/venia/OptMLProject/finetuning/finetuning.py", line 256, in <module>
    main()  # pragma: no cover
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/scratch/venia/python/miniconda3/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/scratch/venia/OptMLProject/finetuning/finetuning.py", line 207, in main
    print(f"Epoch: {epoch}, Loss:  {loss.item()}")
KeyboardInterrupt